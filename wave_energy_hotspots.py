# -*- coding: utf-8 -*-
"""mini.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/147cJPJyjXhkV64YEthAfqiNfaPc4LA47
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import cv2
import os
import numpy as np
import matplotlib.pyplot as plt
from skimage import io
from skimage.filters import unsharp_mask
import tifffile as tiff
from skimage import data
from skimage.color import rgb2hsv
from skimage import filters
from skimage.color import rgb2gray

import pandas as pd
from sklearn.cluster import KMeans
from numpy.fft import fft2, ifft2
from scipy.signal import gaussian, convolve2d
from math import log10, sqrt
from mpl_toolkits import mplot3d
# %matplotlib inline
from IPython.display import HTML

image = cv2.imread("/content/drive/MyDrive/Colab Notebooks/02.tif", 1)\

def blur(img, kernel_size = 3):
	dummy = np.copy(img)
	h = np.eye(kernel_size) / kernel_size
	dummy = convolve2d(dummy, h, mode = 'valid')
	return dummy

def add_gaussian_noise(img, sigma):
	gauss = np.random.normal(0, sigma, np.shape(img))
	noisy_img = img + gauss
	noisy_img[noisy_img < 0] = 0
	noisy_img[noisy_img > 255] = 255
	return noisy_img

def wiener_filter(img, kernel, K):
	kernel /= np.sum(kernel)
	dummy = np.copy(img)
	dummy = fft2(dummy)
	kernel = fft2(kernel, s = img.shape)
	kernel = np.conj(kernel) / (np.abs(kernel) ** 2 + K)
	dummy = dummy * kernel
	dummy = np.abs(ifft2(dummy))
	return dummy

def gaussian_kernel(kernel_size = 3):
	h = gaussian(kernel_size, kernel_size / 3).reshape(kernel_size, 1)
	h = np.dot(h, h.transpose())
	h /= np.sum(h)
	return h

def rgb2gray(rgb):
	return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])


if __name__ == '__main__':

	img = rgb2gray(image)

	# Blur the image
	blurred_img = blur(img, kernel_size = 7)

	# Add Gaussian noise
	noisy_img = add_gaussian_noise(blurred_img, sigma = 20)

	# Apply Wiener Filter
	kernel = gaussian_kernel(5)
	filtered_img = wiener_filter(noisy_img, kernel, K = 10)

	# Display results
	display = [img, blurred_img, noisy_img, filtered_img]
	label = ['Original Image', 'Motion Blurred Image', 'Motion Blurring + Gaussian Noise', 'Wiener Filter applied']

	fig = plt.figure(figsize=(12, 10))

	for i in range(len(display)):
		fig.add_subplot(2, 2, i+1)
		plt.imshow(display[i], cmap = 'gray')
		plt.title(label[i])

	plt.show()

def PSNR(original, compressed):
    mse = np.mean((original - compressed) ** 2)
    if(mse == 0):  # MSE is zero means no noise is present in the signal .
                  # Therefore PSNR have no importance.
        return 100
    max_pixel = 255.0
    psnr = 20 * log10(max_pixel / sqrt(mse))
    return psnr

original = blurred_img
compressed =filtered_img
value = PSNR(original, compressed)
msre = np.sqrt(np.mean((original - compressed)**2))
print("PSNR value is {value} dB")
print("MSE value is {msre} dB")

# Set path to the folder containing the images in Google Drive
image_folder_path = '/content/drive/MyDrive/Colab Notebooks/sar'

# Define a function to import multiple images
def import_images(image_folder_path):
  images = []
  for file_name in os.listdir(image_folder_path):
    # Check if file is an image
    if file_name.endswith('.tif'):
      img = cv2.imread(os.path.join(image_folder_path, file_name))
      # Append image to list
      images.append(img)
  return images

# Call the function to import images
images = import_images(image_folder_path)

# Print the number of images imported
print(f'Number of images imported: {len(images)}')

original_image = image
img=cv2.cvtColor(original_image,cv2.COLOR_BGR2RGB)
vectorized = img.reshape((-1,3))
vectorized = np.float32(vectorized)
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
K = 3
attempts=10
ret,label,center=cv2.kmeans(vectorized,K,None,criteria,attempts,cv2.KMEANS_PP_CENTERS)
center = np.uint8(center)
res = center[label.flatten()]
result_image = res.reshape((img.shape))
figure_size = 15
plt.figure(figsize=(figure_size,figure_size))
'''plt.subplot(1,2,1),plt.imshow(img)
plt.title('Original Image'), plt.xticks([]), plt.yticks([])'''
plt.subplot(1,2,1),plt.imshow(result_image)
plt.title('Segmented Image when K = %i' % K), plt.xticks([]), plt.yticks([])
plt.show()

x=np.uint8(0)
y=np.uint8(255)
im_gray = cv2.cvtColor(result_image, cv2.COLOR_BGR2GRAY)
_, mask = cv2.threshold(im_gray, thresh=75, maxval=255, type=cv2.THRESH_BINARY)
im_thresh_gray = cv2.bitwise_and(im_gray, mask)
mask3 = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
l=[]
for i in mask3:
  k=[]
  for j in i:
    if j[0]==mask3[0][0][0]:
      k.append([x]*3)
    else:
      k.append([y]*3)
  l.append(k)
l=np.array(l)
im_gray = cv2.cvtColor(l, cv2.COLOR_BGR2GRAY)
_, mask = cv2.threshold(im_gray, thresh=75, maxval=255, type=cv2.THRESH_BINARY)
im_thresh_gray = cv2.bitwise_and(im_gray, mask)
mask3 = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
im_thresh_color = cv2.bitwise_and(image, mask3)
plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.imshow(im_thresh_color)
plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 2)
plt.imshow(mask3)
im=image
im1=image

x=np.uint8(0)
y=np.uint8(255)
edges = cv2.Canny(im_thresh_color,100,200)
edge=[]
print(np.size(edges, 0),np.size(edges, 1))
k=0
for i in edges:
  l=[]
  k=k+1
  for j in i:
    if j==0:
      l.append([x]*3)
    else:
      l.append([y]*3)
  edge.append(l)
edges= np.array(edge)
dest_or = cv2.bitwise_and(edges, mask3, mask = None)
plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.imshow(dest_or)
plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 2)
plt.imshow(edges)

x=np.uint8(0)
y=np.uint8(255)
edges = cv2.Canny(mask3,100,200)
edge=[]
print(np.size(edges, 0),np.size(edges, 1))
k=0
for i in edges:
  l=[]
  k=k+1
  for j in i:
    if j==0:
      l.append([x]*3)
    else:
      l.append([y]*3)
  edge.append(l)
edges= np.array(edge)
'''dest_or = cv2.bitwise_and(edges, mask3, mask = None)
'''
plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.imshow(edges)

x=np.uint8(0)
y=np.uint8(255)
l=[]
for i in edges:
  k=[]
  for j in i:
    if j[0]==y:
      k.append([x]*3)
    else:
      k.append([y]*3)
  l.append(k)
l=np.array(l)
plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.imshow(l)

dest_or = cv2.bitwise_and(dest_or, l, mask = None)
plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.imshow(dest_or)

n_white_pix = np.sum(dest_or[112:][14:] == 255)
print('Number of white pixels:', n_white_pix)

x=[]
y=[]
for i in range(np.size(dest_or,0)):
  for j in range(np.size(dest_or,1)):
    if dest_or[i][j][0]==255:
      x.append(i)
      y.append(j)
X=np.column_stack((x, y))
print(X)

wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X)

plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
plt.title('Clusters of density hotspots')
plt.legend()
plt.show()
print((kmeans.cluster_centers_[:, 0][0], kmeans.cluster_centers_[:, 1][0]))
for i in range(3):
  im1 = cv2.circle(im1, (int(kmeans.cluster_centers_[:, 1][i]), int(kmeans.cluster_centers_[:, 0][i])), 10, (255, 0, 0), 2)
plt.figure(figsize=(5,5))
plt.subplot(1,1,1)
plt.imshow(im1)

im2=image
for i in range(3):
  im2 = cv2.circle(im2, (int(kmeans.cluster_centers_[:, 1][i]), int(kmeans.cluster_centers_[:, 0][i])), 100, (255, 0, 0), 20)
plt.figure(figsize=(5,5))
plt.subplot(1,1,1)
plt.imshow(im2)

kmeans = KMeans(n_clusters = 30, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
plt.title('Clusters of density hotspots')
plt.legend()
plt.show()

x= np.linspace(8.132907840065796,8.138363440086254, 30,dtype=float)
y= np.linspace(78.27757662859307,79.27351831812979, 30,dtype=float)
k=[]
for i in range(30):
  l=[]
  for j in range(30):
     l.append(image[i:50][50:i].count(True))
  k.append(l)
z=np.array(k)
fig = plt.figure(figsize=(8,8))
ax = plt.axes(projection='3d')
ax.contour3D(x,y,z,50, cmap='binary')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z');

im3=image
print((kmeans.cluster_centers_[:, 0][0], kmeans.cluster_centers_[:, 1][0]))
for i in range(30):
  if 60<int(kmeans.cluster_centers_[:, 1][i])<140:
    im3 = cv2.circle(im3, (int(kmeans.cluster_centers_[:, 1][i]), int(kmeans.cluster_centers_[:, 0][i])),170, (255, 0, 0),30)
  else:
    im3 = cv2.circle(im3, (int(kmeans.cluster_centers_[:, 1][i]), int(kmeans.cluster_centers_[:, 0][i])),90, (255, 0, 0),20)

plt.figure(figsize=(5,5))
plt.subplot(1,1,1)
plt.imshow(im3)

import os
import cv2
import numpy as np
import tifffile as tiff
from patchify import patchify
from PIL import Image
import sys

sys.path.append('/usr/local/lib/python2.7/site-packages')
from sklearn.preprocessing import MinMaxScaler, StandardScaler
scaler = MinMaxScaler()

root_directory = 'sar/'

patch_size = 128

#Read images from repsective 'images' subdirectory
#As all images are of ddifferent size we have 2 options, either resize or crop
#But, some images are too large and some small. Resizing will change the size of real objects.
#Therefore, we will crop them to a nearest size divisible by 256 and then
#divide all images into patches of 512*512*3.
image_dataset = []
for path, subdirs, files in os.walk(root_directory):
    print(path)
    dirname = path.split(os.path.sep)[-1]
    if dirname == 'sar/buildipatches':   #Find all 'images' directories
        images = os.listdir(path)  #List of all image names in this subdirectory
        for i, image_name in enumerate(images):
            if image_name.endswith(".png"):   #Only read jpg images...

                image = cv2.imread(path+"/"+image_name, 1)  #Read each image as BGR
                SIZE_X = (image.shape[1]//patch_size)*patch_size #Nearest size divisible by our patch size
                SIZE_Y = (image.shape[0]//patch_size)*patch_size #Nearest size divisible by our patch size
                image = Image.fromarray(image)
                image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner
                #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation
                image = np.array(image)

                #Extract patches from each image
                print("Now patchifying image:", path+"/"+image_name)
                patches_img = patchify(image, (patch_size, patch_size, 3), step=patch_size)

                for i in range(patches_img.shape[0]):
                    for j in range(patches_img.shape[1]):

                        single_patch_img = patches_img[i,j,:,:]

                        #Use minmaxscaler instead of just dividing by 255.
                        #single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)


                        #single_patch_img = (single_patch_img.astype('float32')) / 255.
                        #single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds.
                        tiff.imwrite('C:\\Users\\raja\\Desktop\\MAJOR\\SAR\\patches\\' + str(image_name) + '_' + str(i)+str(j)+ ".png", single_patch_img)

print("ENTER LATITUDE AND LONGITUDE LCATIONS OF THE DATA SET")
la=[float(i) for i in input().split()]
lo=[float(i) for i in input().split()]
pix=[int(i) for i in input().split()]
print(la,lo)

for i in range(4):
  print("location of",i+1,"cluster :")
  la1=la[0]+((abs(la[0]-la[1])*(int(kmeans.cluster_centers_[:, 1][i])))/np.size(dest_or,0))
  lo1=lo[1]+((abs(lo[0]-lo[1])*(int(kmeans.cluster_centers_[:, 0][i])))/np.size(dest_or,1))
  print(la1,lo1)

la[1]=la[0]+((2408/2360)*(la[1]-la[0]))
lo[1]=lo[0]-((2560/3002)*(lo[0]-lo[1]))
print(la,lo)

coo=[]
for i in  range(4):
  for j in range(5):
    topl=[la[0]+((i/4)*(la[1]-la[0])),lo[0]-((j/5)*(lo[0]-lo[1]))]
    topl=[round(topl[0],2),round(topl[1],2)]
    botr=[la[0]+(((i+1)/4)*(la[1]-la[0])),lo[0]-(((j+1)/5)*(lo[0]-lo[1]))]
    botr=[round(botr[0],2),round(botr[1],2)]
    coo.append([topl,botr])

len(coo)

pdensity=[]
for i in range(len(images)):
    gradient_magnitude = cv2.Sobel(images[i], cv2.CV_64F, 1, 1, ksize=3)
    WED = (np.sum(gradient_magnitude**2))//(10**6)
    pdensity.append(WED)

density=[]
pddata=[]
for i in range(0,20):
  density.append(str(pdensity[i])+"J/M^2")
avg=sum(pdensity)/len(pdensity)
for x in pdensity:
  pddata.append(str(round((x/avg)*100,3))+"%")
print(pddata)
print(density)

table_data = [['Coordinates','Density','% of avg']]
for m in range(20):
  line="TOP_LEFT  ["+str(coo[m][0][0])+","+str(coo[m][0][1])+"]"+'\n'+"BOTTOM RIGHT ["+str(coo[m][1][0])+","+str(coo[m][1][1])+"]"
  table_data.append([line,density[m],pddata[m]])

table_html = '<table style="border-collapse: collapse; border: 5px solid white;"><tr>{}</tr>{}</table>'
header_row = '<th style="border: 5px solid white; padding: 5px;">{}</th>'
data_row = '<tr>{}</tr>'
data_cell = '<td style="border: 5px solid white; padding: 5px;">{}</td>'

header_html = ''.join(header_row.format(cell) for cell in table_data[0])
data_html = ''.join(data_row.format(''.join(data_cell.format(cell) for cell in row)) for row in table_data[1:])
table_html = table_html.format(header_html, data_html)

HTML(table_html)



a = list(set(density))
a.sort()
res = []
coor=[]
for i in a:
    for j in range(0, len(density)):
        if(density[j] == i):
            res.append(images[j])
            coor.append(coo[j])
density.sort()

import math
fig=plt.figure(figsize=(5,5))
columns=3
rows=math.ceil((len(images)/3))
for i in reversed(range(len(images))):
  plt.imshow(res[i])
  plt.title("Density is: "+str(density[i])+" J/m\u00b3"+"\nCoordinates are: \n"+"Top left: "+str(coor[i][0][0])+", "+str(coor[i][0][1])+"\nBottom Right: "+str(coor[i][1][0])+", "+str(coor[i][1][1]))
  plt.show()

l=[[1,0,1,0,1],[1,0,1,1,1]]
l.count()

#wcss
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()'''
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
plt.title('Clusters of density hotspots')
plt.legend()
plt.show()
plt.figure(figsize=(5,5))
plt.subplot(1,1,1)
plt.imshow(im1)
plt.figure(figsize=(5,5))
plt.subplot(1,1,1)
plt.imshow(im2)'''
#3d
fig = plt.figure(figsize=(8,8))
ax = plt.axes(projection='3d')
ax.contour3D(x,y,z,50, cmap='binary')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

import numpy as np
import matplotlib.pyplot as plt

# Example 2D data
z = np.random.random((10, 10))  # 2D array of random values
x = np.arange(0, 10)
y = np.arange(0, 10)

# Create the heatmap
plt.pcolormesh(x, y, z, cmap='viridis')

# Add a color bar to show the range of values
plt.colorbar()

# Set labels
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('Heatmap')

# Show the plot
plt.show()